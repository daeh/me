# OM features
module add openmind/gcc/5.3.0 # needed to lauch sbatch jobs
module add slurm

# Project specific environments
source "${HOME}/.functions"
source "${HOME}/.projects"

### Command aliases
alias o="/om/user/daeda/software/rmate"
alias zshconfig="vim ~/.zshrc"

### srun alises
alias interact="srun --x11 --cpus-per-task=6 --mem=25G --time=1-00:00:00 --pty zsh"
alias interactqos="srun --x11 --cpus-per-task=2 --mem=4G --time=1-00:00:00 --partition=gablab --pty zsh"
alias interactmin="srun --x11 --mem=4G --time=0-4:00:00 --pty zsh"
alias interactmid="srun --x11 --cpus-per-task=10 --mem=60G --time=1-12:00:00 --pty zsh"
alias interactmax="srun --x11 --cpus-per-task=16 --mem=100G --time=1-12:00:00 --pty zsh"
alias interactquick="srun --x11 --mem=60G -p om_bigmem --pty zsh" # time = 1h. If the requested time limit exceeds the partition's time limit, the job will be left in a PENDING state (possibly indefinitely). The default time limit is the partition's default time limit.
alias interactiaa="srun --x11 --cpus-per-task=25 --mem=30G --time=1-12:00:00 --pty zsh"
alias interactiaaheavy="srun --x11 --cpus-per-task=25 --mem=60G --time=1-12:00:00 --pty zsh"
alias interactiaaquick="srun --x11 --cpus-per-task=25 --mem=60G -p om_bigmem --pty zsh" # time limit is 1h

### Slurm aliases
alias ssque='squeue -u daeda --format="%.18i %.20j %.9P %.2t %.10M %.10l %.7c %.7m %.6D %R"'
alias ssinfo='sinfo -N -o "%N, %c, %C, %e, %E, %G, %m, %T, %z"'
alias ssacctfin="sacct --format=jobid,jobname%40,maxvmsize,maxrss,maxpages,alloccpus,elapsed,exitcode,maxdiskread,maxdiskwrite,state"

### Cluster location aliases
alias omhome="cd /om/user/daeda"
alias om2home="cd /om2/user/daeda"
alias mhhome="cd /mindhive/gablab/u/daeda"