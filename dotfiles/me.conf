a=$(echo ${HOST:-$HOSTNAME} | shasum)
# export BULLETTRAIN_DIR_BG=#${a:0:6}
export TMUX_COLOR=#${a:0:6}



# OM features
module load openmind/gcc/11.1.0 # needed to lauch sbatch jobs
# module load openmind/gcc/12.2.0 # needed to lauch sbatch jobs
# FUTURE::module load openmind8/gcc/12.2.0
module load slurm

### Command aliases
alias o="/home/daeda/me/rmate/bin/rmate"
alias zshconfig="vim ~/.zshrc"

### srun alises
function interact_template () {
	echo "srun --cpus-per-task=6 --mem=25G --time=1-00:00:00 --pty zsh"
}
alias interact="srun --cpus-per-task=6 --mem=25G --time=1-00:00:00 --pty zsh"
alias interactlong="srun --cpus-per-task=6 --mem=25G --time=2-00:00:00 --pty zsh"
alias interactqos="srun --cpus-per-task=2 --mem=4G --time=1-00:00:00 --partition=gablab --pty zsh"
alias interactmin="srun --mem=4G --time=0-4:00:00 --pty zsh"
alias interactmid="srun --cpus-per-task=10 --mem=60G --time=1-12:00:00 --pty zsh"
alias interactmax="srun --cpus-per-task=16 --mem=100G --time=1-12:00:00 --pty zsh"
alias interactquick="srun --mem=60G -p om_bigmem --pty zsh" # time = 1h. If the requested time limit exceeds the partition's time limit, the job will be left in a PENDING state (possibly indefinitely). The default time limit is the partition's default time limit.
alias interactcam="srun --x11 --cpus-per-task=25 --mem=30G --time=1-12:00:00 --pty zsh"
alias interactcamheavy="srun --cpus-per-task=25 --mem=60G --time=1-12:00:00 --pty zsh"
alias interactcamquick="srun --cpus-per-task=25 --mem=60G -p om_bigmem --pty zsh" # time limit is 1h

### Slurm aliases
alias ssque='squeue --user=daeda --format="%32i %20j %.9P %.2t %.10M %.10l %.7C %.7m %R"' ###  %.6D %R
alias ssinfo='sinfo -N -o "%N, %c, %C, %e, %E, %G, %m, %T, %z"'
alias ssacctfin="sacct --format=jobid%20,jobname%30,ReqMem,maxrss,maxvmsize,maxpages,alloccpus,elapsed,exitcode,maxdiskread,maxdiskwrite,maxrssnode,state --units=G"

### Cluster location aliases
alias omhome="cd /om/user/daeda"
alias om2home="cd /om2/user/daeda"
alias mhhome="cd /mindhive/gablab/u/daeda"



############## functions


function tb_conda () {
	
	echo ''
	echo 'loading conda'

	# >>> conda initialize >>>
	# !! Contents within this block are managed by 'conda init' !!
	__conda_setup="$('/om/weka/gablab/daeda/software/anaconda3/bin/conda' 'shell.zsh' 'hook' 2> /dev/null)"
	if [ $? -eq 0 ]; then
		eval "$__conda_setup"
	else
		if [ -f "/om/weka/gablab/daeda/software/anaconda3/etc/profile.d/conda.sh" ]; then
			. "/om/weka/gablab/daeda/software/anaconda3/etc/profile.d/conda.sh"
		else
			export PATH="/om/weka/gablab/daeda/software/anaconda3/bin:$PATH"
		fi
	fi
	unset __conda_setup
	# <<< conda initialize <<<

	echo ''
	echo ">>>conda version:"
	which conda
	conda -V

	conda activate omlab
}

function tb_latex () {
	
	echo ''
	echo 'loading latex'

	export PATH="/om2/user/daeda/software/texlive/bin/x86_64-linux:$PATH"
	echo ''
	echo ''
}

function tb_matlab () {
	echo ''
	echo 'loading matlab functions'

	source /usr/share/Modules/init/zsh ### load the 'module' command for the current shell

	module add mit/matlab/2021b
	alias matlabcl="matlab -nodesktop -nosplash -nodisplay -singleCompThread"
	alias matlabclgreedy="matlab -nodesktop -nosplash -nodisplay"

	echo ''
	echo 'PATH:'
	echo "$PATH"
	echo ''
	echo ''
}

function tb_webppl () {
	echo ''
	echo 'loading webppl functions'

	echo ''
	echo 'loading NVM'

	export NVM_DIR="$HOME/.nvm"
	[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"  # This loads nvm
	[ -s "$NVM_DIR/zsh_completion" ] && \. "$NVM_DIR/zsh_completion"  # This loads nvm bash_completion

	echo ''
	echo 'loading node'

	export PATH="/om/weka/gablab/daeda/software/node-v12.14.0-linux-x64/bin/:$PATH"

	echo ">>>node version:"
	npm version
	echo ">>>WebPPL version:"
	npm list -g webppl
	echo ">>>WebPPL version:"
	webppl --version

	echo ''
	echo 'PATH:'
	echo "$PATH"
	echo ''
	echo ''
}

function tb_fmriprep () {
	echo ''
	echo 'loading fmriprep functions'

	source /usr/share/Modules/init/zsh ### load the 'module' command for the current shell
	module add openmpi/gcc/64/1.8.1
	module add openmind/singularity/3.9.5  ### previously used 3.0.3

	echo "$PATH"
	echo ''
	echo ''
}

function tb_nipype () {
	source /usr/share/Modules/init/zsh ### load the 'module' command for the current shell

	tb_conda
	conda activate ve_nipip

	echo 'loading nipype functions'


	tb_matlab
	echo 'loading matlab'

	# Needed for nipype
	export SPM_PATH="/om2/user/daeda/software/spm12"
	
	### to run ANTS from fmriprep
	module add openmind/singularity/3.9.5

	# module add openmind/gcc/7.5.0
	module add openmind/gcc/11.1.0
	module add openmind/ants/2.1.0-3.8bed08
	module add openmind/afni/2016.03.08
	module add openmind/slicer/4.11.0

	###SYSTEM FSL
	# module add openmind/fsl/5.0.9 

	###CUSTOM FSL
	# FSL Setup -- saxelab custom
	# FSLDIR=/om3/group/saxelab/software/fsl ### 6.0.3
	FSLDIR=/om2/user/daeda/software/fsl ### 6.0.4
	PATH=${FSLDIR}/bin:${PATH}
	export FSLDIR PATH
	. ${FSLDIR}/etc/fslconf/fsl.sh
	
	###SYSTEM freesurfer
	# module add openmind/freesurfer/6.0.0

	###CUSTOM freesurfer
	export FREESURFER_HOME=/om2/user/daeda/software/freesurfer ### 7.1.1
	### set SUBJECTS_DIR for the project
	# export SUBJECTS_DIR=$FREESURFER_HOME/subjects
	export FS_LICENSE='/gablab/p/ADHDER/data/adhder/code/license.txt'
	source $FREESURFER_HOME/SetUpFreeSurfer.sh
	echo "set default number of threads using:"
	echo "export ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS=2"
	echo ""
	echo "remember to >>   export SUBJECTS_DIR=<subjects_dir>   << before use"
	echo ""

	# echo "$PATH"
	# echo ''
	# echo ''

	function nicrash () {
		nipypecli crash ./crash-*.pklz(om[1])
	}
}

function launchkernel_cam_tmux () {
	# "tmuxp_script.yaml in code"
	# "calls launchkernel_cam_torch_interactive() from ~/.functions"
	# "	links current logfile in ~/.tmux_temp_/current_cam_remotelog.txt"
	# "	can be called with launchkernel_cam_head() from ~/.functions"
	# "calls sbatch launch_remote_kernel_torch_interactive.sbatch in code"
	# "	writes ../logs/jupyter-log-currenttunnel.txt"
	# "	can be called with getssh_cam() from ~/.functions"

	tb_conda
	
	cd "/home/daeda/itegb_cam/code/" || exit 

	tmux kill-session -t CAM-torch-kernel

	tmuxp load ./tmuxp_script.yaml
}

function launchkernel_cam_head () {
	tmux_temp_dir="/home/daeda/itegb_cam/logs/.tmux_temp_"
	print "${tmux_temp_dir}/current_cam_remotelog.txt"
	head -n 40 "${tmux_temp_dir}/current_cam_remotelog.txt"
}

function getssh_cam () {
	tail "/home/daeda/itegb_cam/logs/jupyter-log-currenttunnel.txt" || exit
}

function launchkernel_cam_torch_interactive () {
	cd "/home/daeda/itegb_cam/code/" || exit 

	partit='gablab'
	vared -p "partition (gablab|om_bigmem|-none-): " partit

	time='2-00:00:00'
	vared -p "time: " time

	ncpu=6
	vared -p "cpus-per-task: " ncpu

	mem=25
	vared -p "mem: " mem

	print "Intel Xeon E5 : dgx001,dgx002,node017,node[031-077]"
	print "Intel Xeon Gold : node[078-085,087-094,097,098]"
	print "AMD Centos 8 : node[100-116]"
	excludenode="node[031,086,100-116]"
	vared -p "excludenode: " excludenode

	jobname='juptun_cam'
	vared -p "job-name: " jobname

	if [[ -z "${partit}" ]]; then
		partition=""
	else
		partition="--partition=${partit}"
	fi

	scriptfile='launch_remote_kernel_torch_interactive.sbatch'

	slurmout=$(sbatch --job-name=$jobname --cpus-per-task=$ncpu --mem-per-cpu=${mem}GB --time=$time --exclude=$"{excludenode}" $partition $scriptfile)

	print "sbatch returned: vvv"
	print "${slurmout}"
	print "sbatch returned: ^^^"
	print ""

	numeric_string_only=${slurmout//[^0-9]/}
	logfile="../logs/jupyter-log-${numeric_string_only}.txt"
	tmux_temp_dir="/home/daeda/itegb_cam/logs/.tmux_temp_"
	logfilelink="${tmux_temp_dir}/current_cam_remotelog.txt"

	### store current logfile location
	if [[ ! -e $tmux_temp_dir ]]; then
		mkdir $tmux_temp_dir
	# elif [[ ! -d $dir ]]; then
	# 	echo "$Message" 1>&2
	fi

	echo "Waiting for ${logfile}   ..."
	# while ! [ -f "${logfile}" ]; do
	# 	printf "#"
	# 	sleep 2
	# done

	while [[ ! -f "${logfile}" ]]; do
		printf "#"
		sleep 2
	done
	
	[[ -f "${logfilelink}" ]] && rm "${logfilelink}"
	ln "${logfile}" "${logfilelink}"

	print "\n>>> launchkernel_cam_torch_interactive launching sbatch $scriptfile <<<"
	# print "${logfile}"
	#echo "testln-- ${logfilelink}"
	print "logfile: ${logfile}\n-------------------\n"

	
	tail -f -n 70 "${logfile}"
}

function launchkernel_cam () {
	cd "/home/daeda/itegb_cam/code/" || exit 
	slurmout="$(sbatch launch_remote_kernel.sbatch)"
	echo "${slurmout}"
	numeric_string_only=${slurmout//[^0-9]/}
	# echo "${numeric_string_only}"
	logfile="../logs/jupyter-log-${numeric_string_only}.txt"
	tmux_temp_dir="/home/daeda/itegb_cam/logs/.tmux_temp_"
	logfilelink="${tmux_temp_dir}/current_cam_remotelog.txt"

	### store current logfile location
	if [[ ! -e $tmux_temp_dir ]]; then
		mkdir $tmux_temp_dir
	# elif [[ ! -d $dir ]]; then
	# 	echo "$Message" 1>&2
	fi

	while ! [ -f "${logfile}" ]; do
		printf "#"
		sleep 2
	done
	
	[[ -f "${logfilelink}" ]] && rm "${logfilelink}"
	ln "${logfile}" "${logfilelink}"

	echo "test"
	echo "${logfile}"
	echo "testln-- ${logfilelink}"
	echo "logfile: ${logfile}"
	tail -f -n 70 "${logfile}"
	tail -f -n 70 "${logfile}"
}

function UNUSED_launchkernel_iaa_cmdstan () {
	cd "/home/daeda/itegb_cam/code/" || exit 
	slurmout="$(sbatch launch_remote_kernel_cmdstan.sbatch)"

	echo "sbatch returned: vvv"
	echo "${slurmout}"
	echo "sbatch returned: ^^^"
	echo ""

	numeric_string_only=${slurmout//[^0-9]/}
	logfile="../logs/jupyter-log-${numeric_string_only}.txt"
	tmux_temp_dir="/home/daeda/itegb_cam/logs/.tmux_temp_"
	logfilelink="${tmux_temp_dir}/current_iaacmdstan_remotelog.txt"

	### store current logfile location
	if [[ ! -e $tmux_temp_dir ]]; then
		mkdir $tmux_temp_dir
	# elif [[ ! -d $dir ]]; then
	# 	echo "$Message" 1>&2
	fi

	while ! [ -f "${logfile}" ]; do
		printf "#"
		sleep 2
	done
	
	#[[ -f "${logfilelink}" ]] && rm "${logfilelink}"
	#ln "${logfile}" "${logfilelink}"

	echo ""
	echo ">>> UNUSED_launchkernel_iaa_cmdstan launching sbatch launch_remote_kernel_cmdstan.sbatch <<<"
	echo "${logfile}"
	#echo "testln-- ${logfilelink}"
	echo "logfile: ${logfile}"
	tail -f -n 70 "${logfile}"
}

function launchkernel_cam_torch () {
	cd "/home/daeda/itegb_cam/code/" || exit 
	slurmout="$(sbatch launch_remote_kernel_torch.sbatch)"

	echo "sbatch returned: vvv"
	echo "${slurmout}"
	echo "sbatch returned: ^^^"
	echo ""

	numeric_string_only=${slurmout//[^0-9]/}
	logfile="../logs/jupyter-log-${numeric_string_only}.txt"
	tmux_temp_dir="/home/daeda/itegb_cam/logs/.tmux_temp_"
	logfilelink="${tmux_temp_dir}/current_cam_remotelog.txt"

	### store current logfile location
	if [[ ! -e $tmux_temp_dir ]]; then
		mkdir $tmux_temp_dir
	# elif [[ ! -d $dir ]]; then
	# 	echo "$Message" 1>&2
	fi

	while ! [ -f "${logfile}" ]; do
		printf "#"
		sleep 2
	done
	
	#[[ -f "${logfilelink}" ]] && rm "${logfilelink}"
	#ln "${logfile}" "${logfilelink}"

	echo ""
	echo ">>> launchkernel_cam_torch launching sbatch launch_remote_kernel_torch.sbatch <<<"
	echo "${logfile}"
	#echo "testln-- ${logfilelink}"
	echo "logfile: ${logfile}"
	tail -f -n 70 "${logfile}"
}

function launchkernel_cam_bigmem () {
	cd "/home/daeda/itegb_cam/code/" || exit 
	slurmout="$(sbatch launch_remote_kernel_bigmem.sbatch)"
	echo "${slurmout}"
	numeric_string_only=${slurmout//[^0-9]/}
	# echo "${numeric_string_only}"
	logfile="../logs/jupyter-log-${numeric_string_only}.txt"
	while ! [ -f "${logfile}" ]; do
		printf "#"
		sleep 2
	done
	tail -f -n 70 "${logfile}"
}





function launchkernel_tmux_adhder () {
	# "tmuxp_script.yaml in code"
	# "calls launchkernel_adhder() from ~/.functions"
	# "	links current logfile in ~/.tmux_temp_/current_remotelog_adhder.txt"
	# "	can be called with launchkernel_head_adhder() from ~/.functions"
	# "calls sbatch launch_remote_kernel.sbatch in code"
	# "	writes ../logs/jupyter-log-currenttunnel.txt"
	# "	can be called with getssh_cam() from ~/.functions"

	tb_conda
	
	cd "/gablab/p/ADHDER/data/adhder/code/analysis/" || exit

	tmux kill-session -t adhdNipype-kernel

	tmuxp load ./tmuxp_script.yaml
}
function launchkernel_adhder () {
	cd "/gablab/p/ADHDER/data/adhder/code/analysis/" || exit
	slurmout="$(sbatch launch_remote_kernel.sbatch)"

	echo "sbatch returned: vvv"
	echo "${slurmout}"
	echo "sbatch returned: ^^^"
	echo ""

	numeric_string_only=${slurmout//[^0-9]/}
	logfile="../../logs_sbatch/jupyter-log-${numeric_string_only}.txt"
	tmux_temp_dir="/home/daeda/itegb_cam/logs/.tmux_temp_"
	logfilelink="${tmux_temp_dir}/current_remotelog_adhder.txt"

	### store current logfile location
	if [[ ! -e $tmux_temp_dir ]]; then
		mkdir $tmux_temp_dir
	# elif [[ ! -d $dir ]]; then
	# 	echo "$Message" 1>&2
	fi

	while ! [ -f "${logfile}" ]; do
		printf "#"
		sleep 2
	done
	
	#[[ -f "${logfilelink}" ]] && rm "${logfilelink}"
	#ln "${logfile}" "${logfilelink}"

	echo ""
	echo ">>> launchkernel_adhder launching sbatch launch_remote_kernel.sbatch <<<"
	echo "${logfile}"
	#echo "testln-- ${logfilelink}"
	echo "logfile: ${logfile}"
	tail -f -n 70 "${logfile}"
}

############## projects


### Project-specific location aliases
alias projadhd="cd /gablab/p/ADHDER/data/adhder"
alias projadhdb="cd /gablab/p/ADHDER/data/adhder/behavioral"
alias projadhdc="cd /gablab/p/ADHDER/data/adhder/code"
alias projadhdsource="cd /storage/gablab001/data/dicoms/adhder"

alias projiaa="cd /om/user/daeda/ite_iaa/ite_gb_inverseappraisal"
alias projiaac="cd /om/user/daeda/ite_iaa/ite_gb_inverseappraisal/code"
alias projiaadata="cd /om2/user/daeda/iaa_dataout"

function env_adhder () {
	# Set command line prompt
	export project_name="ADHDER"

	# Call appropriate functions from ~/.functions
	# source "$HOME/.functions"
	export SUBJECTS_DIR='/gablab/p/ADHDER/data/adhder/derivatives/freesurfer'
	# notifstr='freesurfer SUBJECTS_DIR set to ${SUBJECTS_DIR}'
	# echo "${notifstr}"
	
	tb_nipype

	# Set directory aliases
	alias phome="/gablab/p/ADHDER/data/adhder/"
	cd "/gablab/p/ADHDER/data/adhder" || exit
}

function env_fmriprep () {
	# Set command line prompt
	export project_name="FMRIPREP"

	# Call appropriate functions from ~/.functions
	# source "$HOME/.functions"
	tb_fmriprep

	# Set directory aliases
	alias phome="/gablab/p/ADHDER/data/adhder/"
}

function env_cam () {
	# exit on error
	# set -e

	# Set command line prompt
	export project_name="ITEGB-CAM"

	# Call appropriate functions from ~/.functions
	# source "$HOME/.functions"
	tb_webppl
	tb_latex
	tb_conda

	# Set directory aliases
	alias phome="/home/daeda/itegb_cam"
	cd "/home/daeda/itegb_cam" || exit
	host_save=$HOST

	conda activate ve_cam
	export HOST=$host_save
	cd code || exit
}


