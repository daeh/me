a=$(echo ${HOST:-$HOSTNAME} | shasum)
# export BULLETTRAIN_DIR_BG=#${a:0:6}
export TMUX_COLOR=#${a:0:6}



# OM features
module add openmind/gcc/5.3.0 # needed to lauch sbatch jobs
module add slurm

# Project specific environments
source "${HOME}/.functions"
source "${HOME}/.projects"

### Command aliases
alias o="/om/user/daeda/software/rmate"
alias zshconfig="vim ~/.zshrc"

### srun alises
alias interact="srun --x11 --cpus-per-task=6 --mem=25G --time=1-00:00:00 --pty zsh"
alias interactqos="srun --x11 --cpus-per-task=2 --mem=4G --time=1-00:00:00 --partition=gablab --pty zsh"
alias interactmin="srun --x11 --mem=4G --time=0-4:00:00 --pty zsh"
alias interactmid="srun --x11 --cpus-per-task=10 --mem=60G --time=1-12:00:00 --pty zsh"
alias interactmax="srun --x11 --cpus-per-task=16 --mem=100G --time=1-12:00:00 --pty zsh"
alias interactquick="srun --x11 --mem=60G -p om_bigmem --pty zsh" # time = 1h. If the requested time limit exceeds the partition's time limit, the job will be left in a PENDING state (possibly indefinitely). The default time limit is the partition's default time limit.
alias interactiaa="srun --x11 --cpus-per-task=25 --mem=30G --time=1-12:00:00 --pty zsh"
alias interactiaaheavy="srun --x11 --cpus-per-task=25 --mem=60G --time=1-12:00:00 --pty zsh"
alias interactiaaquick="srun --x11 --cpus-per-task=25 --mem=60G -p om_bigmem --pty zsh" # time limit is 1h

### Slurm aliases
alias ssque='squeue -u daeda --format="%.18i %.20j %.9P %.2t %.10M %.10l %.7c %.7m %.6D %R"'
alias ssinfo='sinfo -N -o "%N, %c, %C, %e, %E, %G, %m, %T, %z"'
alias ssacctfin="sacct --format=jobid,jobname%40,maxvmsize,maxrss,maxpages,alloccpus,elapsed,exitcode,maxdiskread,maxdiskwrite,state"

### Cluster location aliases
alias omhome="cd /om/user/daeda"
alias om2home="cd /om2/user/daeda"
alias mhhome="cd /mindhive/gablab/u/daeda"



############## functions



#! /bin/zsh

function tb_conda()
{
	
	echo ''
	echo 'loading conda'

	# >>> conda initialize >>>
	# !! Contents within this block are managed by 'conda init' !!
	__conda_setup="$('/om/user/daeda/software/anaconda3/bin/conda' 'shell.zsh' 'hook' 2> /dev/null)"
	if [ $? -eq 0 ]; then
		eval "$__conda_setup"
	else
		if [ -f "/om/user/daeda/software/anaconda3/etc/profile.d/conda.sh" ]; then
			. "/om/user/daeda/software/anaconda3/etc/profile.d/conda.sh"
		else
			export PATH="/om/user/daeda/software/anaconda3/bin:$PATH"
		fi
	fi
	unset __conda_setup
	# <<< conda initialize <<<

	echo ''
	echo ">>>conda version:"
	which conda
	conda -V

	conda activate omlab
}

function tb_matlab()
{
	echo ''
	echo 'loading matlab functions'

	# source /usr/share/Modules/init/bash
	source /usr/share/Modules/init/zsh

	module add mit/matlab/2019a
	alias matlabcl="matlab -nodesktop -nosplash -nodisplay -singleCompThread"
	alias matlabclgreedy="matlab -nodesktop -nosplash -nodisplay"

	echo ''
	echo 'PATH:'
	echo "$PATH"
	echo ''
	echo ''
}

function tb_webppl()
{
	echo ''
	echo 'loading webppl functions'

	echo ''
	echo 'loading NVM'

	export NVM_DIR="$HOME/.nvm"
	[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"  # This loads nvm
	[ -s "$NVM_DIR/zsh_completion" ] && \. "$NVM_DIR/zsh_completion"  # This loads nvm bash_completion

	echo ''
	echo 'loading node'

	# export PATH="/om2/user/daeda/software/node-v6.10.3-linux-x64/bin/:$PATH"
	# export PATH="/om/user/daeda/software/node-v10.16.3-linux-x64/bin/:$PATH"
	export PATH="/om/user/daeda/software/node-v12.14.0-linux-x64/bin/:$PATH"

	echo ">>>node version:"
	npm version
	echo ">>>WebPPL version:"
	npm list -g webppl
	echo ">>>WebPPL version:"
	webppl --version

	tb_conda

	echo ''
	echo 'PATH:'
	echo "$PATH"
	echo ''
	echo ''
}

function tb_fmriprep()
{
	echo ''
	echo 'loading fmriprep functions'

	# source /usr/share/Modules/init/bash
	source /usr/share/Modules/init/zsh

	module add openmpi/gcc/64/1.8.1
	module add openmind/singularity/3.4.1  ### previously used 3.0.3

	echo "$PATH"
	echo ''
	echo ''
}

function tb_nipype()
{
	# load the 'module' command for the current shell
	# source /usr/share/Modules/init/bash
	source /usr/share/Modules/init/zsh

	tb_conda
	echo 'loading nipype functions'

	conda activate niconda

	tb_matlab

	# Needed for nipype
	export SPM_PATH="/om2/user/daeda/software/spm12"
	
	module add openmind/gcc/5.3.0
	module add openmind/ants/2.1.0-3.8bed08
	module add openmind/afni/2016.03.08
	module add openmind/slicer/4.6.2

	###SYSTEM FSL
	# module add openmind/fsl/5.0.9 

	###CUSTOM FSL
	# FSL Setup -- saxelab custom
	FSLDIR=/om3/group/saxelab/software/fsl
	PATH=${FSLDIR}/bin:${PATH}
	export FSLDIR PATH
	. ${FSLDIR}/etc/fslconf/fsl.sh
	
	###SYSTEM freesurfer
	# module add openmind/freesurfer/6.0.0

	###CUSTOM freesurfer
	export FREESURFER_HOME=/om2/user/daeda/software/freesurfer ### 6.0.1
	source $FREESURFER_HOME/SetUpFreeSurfer.sh
	export FS_LICENSE='/gablab/p/ADHDER/data/adhder/code/license.txt'
	echo "set default number of threads using:"
	echo "export ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS=2"
	echo ""
	echo "remember to >>export FREESURFER_HOME=<subjects_dir><< before use"
	echo ""

	echo "$PATH"
	echo ''
	echo ''

	function nicrash()
	{
		#echo "The script you are running has basename `basename $0`, dirname `dirname $0`"
		#echo "The present working directory is `pwd`"
		echo ''
		dir=$(pwd)
		echo "dir== $dir"
		unset -v latest
		  for file in "$dir"/crash-*.pklz; do
		  [[ $file -nt $latest ]] && latest=$file
		done

		echo "nipypecli crash $latest"

		nipypecli crash "$latest"
	}
}

function launchkernel_iaa_tmux()
{
	# "tmuxp_script.yaml in code"
	# "calls launchkernel_iaa() from ~/.functions"
	# "	links current logfile in ~/.tmux_temp_/current_iaa_remotelog.txt"
	# "	can be called with launchkernel_iaa_head() from ~/.functions"
	# "calls sbatch launch_remote_kernel.sbatch in code"
	# "	writes ../logs/jupyter-log-currenttunnel.txt"
	# "	can be called with getssh_iaa() from ~/.functions"

	tb_conda
	
	cd "/om/user/daeda/ite_iaa/ite_gb_inverseappraisal/code/" || exit 

	tmux kill-session -t IAA-kernel

	tmuxp load ./tmuxp_script.yaml
}
function launchkernel_iaacmdstan_tmux()
{
	# "tmuxp_script.yaml in code"
	# "calls launchkernel_iaa() from ~/.functions"
	# "	links current logfile in ~/.tmux_temp_/current_iaa_remotelog.txt"
	# "	can be called with launchkernel_iaa_head() from ~/.functions"
	# "calls sbatch launch_remote_kernel.sbatch in code"
	# "	writes ../logs/jupyter-log-currenttunnel.txt"
	# "	can be called with getssh_iaa() from ~/.functions"

	tb_conda
	
	cd "/om/user/daeda/ite_iaa/ite_gb_inverseappraisal/code/" || exit 

	tmux kill-session -t IAAcmdstan-kernel

	tmuxp load ./tmuxp_script_cmdstan.yaml
}

function launchkernel_iaa_head()
{
	tmux_temp_dir="/om/user/daeda/.tmux_temp_"
	echo "${tmux_temp_dir}/current_iaa_remotelog.txt"
	nano "${tmux_temp_dir}/current_iaa_remotelog.txt"
}

function getssh_iaa()
{
	tail "/om/user/daeda/ite_iaa/ite_gb_inverseappraisal/logs/jupyter-log-currenttunnel.txt" || exit
}

function launchkernel_iaa()
{
	cd "/om/user/daeda/ite_iaa/ite_gb_inverseappraisal/code/" || exit 
	slurmout="$(sbatch launch_remote_kernel.sbatch)"
	echo "${slurmout}"
	numeric_string_only=${slurmout//[^0-9]/}
	# echo "${numeric_string_only}"
	logfile="../logs/jupyter-log-${numeric_string_only}.txt"
	tmux_temp_dir="/om/user/daeda/.tmux_temp_"
	logfilelink="${tmux_temp_dir}/current_iaa_remotelog.txt"

	### store current logfile location
	if [[ ! -e $tmux_temp_dir ]]; then
		mkdir $tmux_temp_dir
	# elif [[ ! -d $dir ]]; then
	# 	echo "$Message" 1>&2
	fi

	while ! [ -f "${logfile}" ]; do
		printf "#"
		sleep 2
	done
	
	if [[ -f "${logfilelink}" ]] && rm "${logfilelink}"
	ln "${logfile}" "${logfilelink}"

	echo "test"
	echo "${logfile}"
	echo "testln-- ${logfilelink}"
	echo "logfile: ${logfile}"
	tail -f -n 70 "${logfile}"
		tail -f -n 70 "${logfile}"
}

function launchkernel_iaa_cmdstan()
{
	cd "/om/user/daeda/ite_iaa/ite_gb_inverseappraisal/code/" || exit 
	slurmout="$(sbatch launch_remote_kernel_cmdstan.sbatch)"
	echo "${slurmout}"
	numeric_string_only=${slurmout//[^0-9]/}
	# echo "${numeric_string_only}"
	logfile="../logs/jupyter-log-${numeric_string_only}.txt"
	tmux_temp_dir="/om/user/daeda/.tmux_temp_cmdstan_"
	logfilelink="${tmux_temp_dir}/current_iaacmdstan_remotelog.txt"

	### store current logfile location
	if [[ ! -e $tmux_temp_dir ]]; then
		mkdir $tmux_temp_dir
	# elif [[ ! -d $dir ]]; then
	# 	echo "$Message" 1>&2
	fi

	while ! [ -f "${logfile}" ]; do
		printf "#"
		sleep 2
	done
	
	if [[ -f "${logfilelink}" ]] && rm "${logfilelink}"
	ln "${logfile}" "${logfilelink}"

	echo "test"
	echo "${logfile}"
	echo "testln-- ${logfilelink}"
	echo "logfile: ${logfile}"
	tail -f -n 70 "${logfile}"
}

function launchkernel_iaa_bigmem()
{
	cd "/om/user/daeda/ite_iaa/ite_gb_inverseappraisal/code/" || exit 
	slurmout="$(sbatch launch_remote_kernel_bigmem.sbatch)"
	echo "${slurmout}"
	numeric_string_only=${slurmout//[^0-9]/}
	# echo "${numeric_string_only}"
	logfile="../logs/jupyter-log-${numeric_string_only}.txt"
	while ! [ -f "${logfile}" ]; do
		printf "#"
		sleep 2
	done
	tail -f -n 70 "${logfile}"
}

function launchkernel_adhder()
{
	cd "/gablab/p/ADHDER/data/adhder/code/analysis/" || exit 
	slurmout="$(sbatch launch_remote_kernel.sbatch)"
	echo "${slurmout}"
	numeric_string_only=${slurmout//[^0-9]/}
	# echo "${numeric_string_only}"
	logfile="logs/jupyter-log-${numeric_string_only}.txt"
	while ! [ -f "${logfile}" ]; do
		printf "#"
		sleep 2
	done
	tail -f -n 70 "${logfile}"
}

############## projects


#! /usr/bin/env zsh

### Project-specific location aliases
alias projadhd="cd /gablab/p/ADHDER/data/adhder"
alias projadhdc="cd /gablab/p/ADHDER/data/adhder/behavioral"
alias projadhdc="cd /gablab/p/ADHDER/data/adhder/code"
alias projadhdsource="cd /storage/gablab001/data/dicoms/adhder"

alias projiaa="cd /om/user/daeda/ite_iaa/ite_gb_inverseappraisal"
alias projiaac="cd /om/user/daeda/ite_iaa/ite_gb_inverseappraisal/code"
alias projiaadata="cd /om2/user/daeda/iaa_dataout"

function env_adhder()
{
	# Set command line prompt
	export project_name="ADHDER"

	# Call appropriate functions from ~/.functions
	source "$HOME/.functions"
	tb_nipype
	export SUBJECTS_DIR='/gablab/p/ADHDER/data/adhder/derivatives/freesurfer'
	echo 'freesurfer ${SUBJECTS_DIR} set to /gablab/p/ADHDER/data/adhder/derivatives/freesurfer'

	# Set directory aliases
	alias phome="/gablab/p/ADHDER/data/adhder/"
	cd /gablab/p/ADHDER/data/adhder || exit
}

function env_fmriprep()
{
	# Set command line prompt
	export project_name="FMRIPREP"

	# Call appropriate functions from ~/.functions
	source "$HOME/.functions"
	tb_fmriprep

	# Set directory aliases
	alias phome="/gablab/p/ADHDER/data/adhder/"
}

function env_iaa()
{
	# Set command line prompt
	export project_name="ITE_IAA"

	# Call appropriate functions from ~/.functions
	source "$HOME/.functions"
	tb_webppl

	# Set directory aliases
	alias phome="/om/user/daeda/ite_iaa/ite_gb_inverseappraisal"
	cd "/om/user/daeda/ite_iaa/ite_gb_inverseappraisal" || exit
	host_save=$HOST
	# conda activate envs/default
	conda activate ve_iaa_pyro
	export HOST=$host_save
	cd code || exit
}
function env_iaa_cmdstan()
{
	# Set command line prompt
	export project_name="ITE_IAAcmdstan"

	# Call appropriate functions from ~/.functions
	source "$HOME/.functions"
	tb_webppl

	# Set directory aliases
	alias phome="/om/user/daeda/ite_iaa/ite_gb_inverseappraisal"
	cd "/om/user/daeda/ite_iaa/ite_gb_inverseappraisal" || exit
	host_save=$HOST
	conda activate ve_iaa_cmdstanpy #envs/pyro
	export HOST=$host_save
	cd code || exit
}


